{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footwear Impression Matching - Visualization and Analysis\n",
    "\n",
    "This notebook provides visualization and analysis tools for the footwear impression matching system. It allows you to:\n",
    "1. Explore the dataset\n",
    "2. Visualize augmentations\n",
    "3. Analyze model results\n",
    "4. Examine failure cases\n",
    "\n",
    "Before running this notebook, make sure you have processed the dataset and trained a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from data.dataloader import get_transforms, ShoeImpressDataset\n",
    "from data.augmentation import FootwearAugmenter\n",
    "from models.network import FootwearMatchingNetwork\n",
    "from utils.common import calculate_metrics, plot_precision_recall_curve, plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration\n",
    "\n",
    "Let's start by exploring the dataset structure and visualizing some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths (modify as needed)\n",
    "data_dir = '../data/processed'\n",
    "train_csv = os.path.join(data_dir, 'train_pairs.csv')\n",
    "val_csv = os.path.join(data_dir, 'val_pairs.csv')\n",
    "\n",
    "# Load pair information\n",
    "train_pairs = pd.read_csv(train_csv)\n",
    "val_pairs = pd.read_csv(val_csv)\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "# Class distribution\n",
    "train_pos = (train_pairs['label'] == 1).sum()\n",
    "train_neg = (train_pairs['label'] == 0).sum()\n",
    "val_pos = (val_pairs['label'] == 1).sum()\n",
    "val_neg = (val_pairs['label'] == 0).sum()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Training: {train_pos} positive ({train_pos/len(train_pairs)*100:.1f}%), {train_neg} negative ({train_neg/len(train_pairs)*100:.1f}%)\")\n",
    "print(f\"Validation: {val_pos} positive ({val_pos/len(val_pairs)*100:.1f}%), {val_neg} negative ({val_neg/len(val_pairs)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize some examples\n",
    "def show_pair(track_path, ref_path, label):\n",
    "    track_img = cv2.imread(track_path)\n",
    "    ref_img = cv2.imread(ref_path)\n",
    "    \n",
    "    # Convert from BGR to RGB\n",
    "    track_img = cv2.cvtColor(track_img, cv2.COLOR_BGR2RGB)\n",
    "    ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    ax[0].imshow(track_img)\n",
    "    ax[0].set_title(\"Track (Crime Scene) Impression\")\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(ref_img)\n",
    "    ax[1].set_title(\"Reference Impression\")\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    match_status = \"Match\" if label == 1 else \"Non-Match\"\n",
    "    plt.suptitle(f\"{match_status} Pair\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some positive pairs\n",
    "positive_pairs = train_pairs[train_pairs['label'] == 1].sample(3)\n",
    "print(\"Positive Pairs (Same footwear):\\n\")\n",
    "for _, row in positive_pairs.iterrows():\n",
    "    print(f\"Track ID: {row['track_id']}, Reference ID: {row['ref_id']}\")\n",
    "    show_pair(row['track_path'], row['ref_path'], row['label'])\n",
    "\n",
    "# Show some negative pairs\n",
    "negative_pairs = train_pairs[train_pairs['label'] == 0].sample(3)\n",
    "print(\"Negative Pairs (Different footwear):\\n\")\n",
    "for _, row in negative_pairs.iterrows():\n",
    "    print(f\"Track ID: {row['track_id']}, Reference ID: {row['ref_id']}\")\n",
    "    show_pair(row['track_path'], row['ref_path'], row['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation Visualization\n",
    "\n",
    "Let's visualize the different augmentation techniques used in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create augmenter\n",
    "augmenter = FootwearAugmenter()\n",
    "\n",
    "# Sample a track image\n",
    "sample_row = train_pairs[train_pairs['label'] == 1].iloc[0]\n",
    "track_path = sample_row['track_path']\n",
    "track_img = cv2.imread(track_path)\n",
    "track_img = cv2.cvtColor(track_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Generate augmentations\n",
    "augmented_batch = augmenter.create_augmentation_batch(track_img, num_variants=8)\n",
    "\n",
    "# Visualize original and augmentations\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i, (img, aug_types) in enumerate(augmented_batch):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\", \".join(aug_types))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Analysis\n",
    "\n",
    "Now, let's load a trained model and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model\n",
    "def load_model(model_path, backbone='resnet50', feature_dim=256, device='cuda'):\n",
    "    # Set device\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model\n",
    "    model = FootwearMatchingNetwork(\n",
    "        backbone=backbone,\n",
    "        pretrained=False,\n",
    "        feature_dim=feature_dim\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "        state_dict = state_dict['model_state_dict']\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "# Set paths (modify as needed)\n",
    "model_path = '../results/checkpoints/best_model.pth'\n",
    "\n",
    "# Try to load the model (will fail if the path is incorrect)\n",
    "try:\n",
    "    model, device = load_model(model_path)\n",
    "    print(f\"Model loaded successfully! Using device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model: {str(e)}\")\n",
    "    print(\"Please set the correct model path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "def evaluate_model(model, val_csv, device, batch_size=32, img_size=512):\n",
    "    # Setup transforms\n",
    "    transform = get_transforms(mode='val', img_size=img_size)\n",
    "    \n",
    "    # Create dataset\n",
    "    val_dataset = ShoeImpressDataset(\n",
    "        val_csv,\n",
    "        transform=transform,\n",
    "        triplet_mode=False,\n",
    "        online_augment=False\n",
    "    )\n",
    "    \n",
    "    # Create data loader\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Evaluating on {len(val_dataset)} validation pairs\")\n",
    "    \n",
    "    # Collect all outputs and targets\n",
    "    all_logits = []\n",
    "    all_similarities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        for track_imgs, ref_imgs, labels, _ in tqdm(val_loader):\n",
    "            # Move to device\n",
    "            track_imgs = track_imgs.to(device)\n",
    "            ref_imgs = ref_imgs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, similarities, _, _ = model(track_imgs, ref_imgs)\n",
    "            \n",
    "            # Collect outputs\n",
    "            all_logits.append(logits.cpu().squeeze())\n",
    "            all_similarities.append(similarities.cpu())\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all outputs and targets\n",
    "    all_logits = torch.cat(all_logits).numpy()\n",
    "    all_similarities = torch.cat(all_similarities).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_logits, all_similarities, all_labels)\n",
    "    \n",
    "    return metrics, all_logits, all_similarities, all_labels\n",
    "\n",
    "# Try to evaluate the model\n",
    "try:\n",
    "    metrics, logits, similarities, labels = evaluate_model(model, val_csv, device)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {metrics['ap']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"AP from Similarity: {metrics['sim_ap']:.4f}\")\n",
    "    \n",
    "    # Plot curves\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['recall'], metrics['precision'], lw=2, marker='.', markersize=3)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve (AP = {metrics[\"ap\"]:.4f})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['fpr'], metrics['tpr'], lw=2, marker='.', markersize=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1.5)  # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis of Failure Cases\n",
    "\n",
    "Let's analyze some failure cases to understand the model's limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze failure cases\n",
    "def analyze_failures(logits, similarities, labels, val_csv, n_samples=5):\n",
    "    # Load validation dataset\n",
    "    val_pairs = pd.read_csv(val_csv)\n",
    "    \n",
    "    # Get probabilities\n",
    "    probas = 1 / (1 + np.exp(-logits))  # Sigmoid\n",
    "    predictions = (probas > 0.5).astype(int)\n",
    "    \n",
    "    # Find failure cases\n",
    "    failure_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    print(f\"Found {len(failure_indices)} failure cases out of {len(labels)} samples ({len(failure_indices)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Analyze false positives and false negatives\n",
    "    false_positives = [(i, probas[i]) for i in failure_indices if predictions[i] == 1 and labels[i] == 0]\n",
    "    false_negatives = [(i, probas[i]) for i in failure_indices if predictions[i] == 0 and labels[i] == 1]\n",
    "    \n",
    "    print(f\"False positives: {len(false_positives)}\")\n",
    "    print(f\"False negatives: {len(false_negatives)}\")\n",
    "    \n",
    "    # Sort by confidence\n",
    "    false_positives.sort(key=lambda x: x[1], reverse=True)\n",
    "    false_negatives.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Visualize top failures\n",
    "    if false_positives:\n",
    "        print(\"\\nTop False Positives (Predicted Match, Actually Different):\")\n",
    "        for i, (idx, conf) in enumerate(false_positives[:n_samples]):\n",
    "            pair = val_pairs.iloc[idx]\n",
    "            print(f\"Pair {i+1}: Track ID {pair['track_id']}, Reference ID {pair['ref_id']}, Confidence: {conf:.4f}\")\n",
    "            show_pair(pair['track_path'], pair['ref_path'], pair['label'])\n",
    "    \n",
    "    if false_negatives:\n",
    "        print(\"\\nTop False Negatives (Predicted Different, Actually Match):\")\n",
    "        for i, (idx, conf) in enumerate(false_negatives[:n_samples]):\n",
    "            pair = val_pairs.iloc[idx]\n",
    "            print(f\"Pair {i+1}: Track ID {pair['track_id']}, Reference ID {pair['ref_id']}, Confidence: {conf:.4f}\")\n",
    "            show_pair(pair['track_path'], pair['ref_path'], pair['label'])\n",
    "\n",
    "# Try to analyze failures\n",
    "try:\n",
    "    analyze_failures(logits, similarities, labels, val_csv)\n",
    "except Exception as e:\n",
    "    print(f\"Analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Visualization\n",
    "\n",
    "Let's visualize the feature spaces to understand how the model separates different impressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract embeddings\n",
    "def extract_embeddings(model, val_csv, device, batch_size=32, img_size=512):\n",
    "    # Setup transforms\n",
    "    transform = get_transforms(mode='val', img_size=img_size)\n",
    "    \n",
    "    # Create dataset\n",
    "    val_dataset = ShoeImpressDataset(\n",
    "        val_csv,\n",
    "        transform=transform,\n",
    "        triplet_mode=False,\n",
    "        online_augment=False\n",
    "    )\n",
    "    \n",
    "    # Create data loader\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings for {len(val_dataset)} samples\")\n",
    "    \n",
    "    # Collect embeddings and metadata\n",
    "    track_embeddings = []\n",
    "    ref_embeddings = []\n",
    "    all_labels = []\n",
    "    track_ids = []\n",
    "    ref_ids = []\n",
    "    \n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        for track_imgs, ref_imgs, labels, meta in tqdm(val_loader):\n",
    "            # Move to device\n",
    "            track_imgs = track_imgs.to(device)\n",
    "            ref_imgs = ref_imgs.to(device)\n",
    "            \n",
    "            # Get features\n",
    "            track_features = model(track_imgs, None, mode='track')\n",
    "            ref_features = model(None, ref_imgs, mode='ref')\n",
    "            \n",
    "            # Global pooling to get embeddings\n",
    "            track_emb = F.adaptive_avg_pool2d(track_features, 1).squeeze(-1).squeeze(-1)\n",
    "            ref_emb = F.adaptive_avg_pool2d(ref_features, 1).squeeze(-1).squeeze(-1)\n",
    "            \n",
    "            # Collect data\n",
    "            track_embeddings.append(track_emb.cpu())\n",
    "            ref_embeddings.append(ref_emb.cpu())\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Collect metadata\n",
    "            track_ids.extend([m['track_id'] for m in meta])\n",
    "            ref_ids.extend([m['ref_id'] for m in meta])\n",
    "    \n",
    "    # Concatenate everything\n",
    "    track_embeddings = torch.cat(track_embeddings).numpy()\n",
    "    ref_embeddings = torch.cat(ref_embeddings).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    return track_embeddings, ref_embeddings, all_labels, track_ids, ref_ids\n",
    "\n",
    "# Try to extract embeddings\n",
    "try:\n",
    "    track_embs, ref_embs, emb_labels, track_ids, ref_ids = extract_embeddings(model, val_csv, device)\n",
    "    print(f\"Extracted {len(track_embs)} embeddings with {track_embs.shape[1]} dimensions\")\n",
    "except Exception as e:\n",
    "    print(f\"Embedding extraction failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize embeddings with dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_embeddings(track_embs, ref_embs, labels, method='tsne', n_components=2):\n",
    "    # Combine track and reference embeddings\n",
    "    all_embs = np.vstack([track_embs, ref_embs])\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "    else:  # PCA\n",
    "        reducer = PCA(n_components=n_components, random_state=42)\n",
    "    \n",
    "    reduced_embs = reducer.fit_transform(all_embs)\n",
    "    \n",
    "    # Split back into track and reference\n",
    "    track_reduced = reduced_embs[:len(track_embs)]\n",
    "    ref_reduced = reduced_embs[len(track_embs):]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot by match/non-match\n",
    "    pos_idx = (labels == 1)\n",
    "    neg_idx = (labels == 0)\n",
    "    \n",
    "    # Plot track points\n",
    "    plt.scatter(track_reduced[pos_idx, 0], track_reduced[pos_idx, 1], c='blue', marker='o', s=50, alpha=0.7, label='Track (Match)')\n",
    "    plt.scatter(track_reduced[neg_idx, 0], track_reduced[neg_idx, 1], c='red', marker='o', s=50, alpha=0.7, label='Track (Non-Match)')\n",
    "    \n",
    "    # Plot reference points\n",
    "    plt.scatter(ref_reduced[pos_idx, 0], ref_reduced[pos_idx, 1], c='cyan', marker='s', s=50, alpha=0.7, label='Reference (Match)')\n",
    "    plt.scatter(ref_reduced[neg_idx, 0], ref_reduced[neg_idx, 1], c='magenta', marker='s', s=50, alpha=0.7, label='Reference (Non-Match)')\n",
    "    \n",
    "    # Draw lines between matching pairs\n",
    "    for i in range(len(track_reduced)):\n",
    "        if labels[i] == 1:  # Only for matching pairs\n",
    "            plt.plot([track_reduced[i, 0], ref_reduced[i, 0]], \n",
    "                     [track_reduced[i, 1], ref_reduced[i, 1]], \n",
    "                     'k-', alpha=0.3)\n",
    "    \n",
    "    plt.title(f'{method.upper()} Visualization of Footwear Impression Embeddings', fontsize=16)\n",
    "    plt.xlabel(f'{method.upper()}-1', fontsize=14)\n",
    "    plt.ylabel(f'{method.upper()}-2', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Try to visualize embeddings\n",
    "try:\n",
    "    print(\"\\nT-SNE Visualization:\")\n",
    "    visualize_embeddings(track_embs, ref_embs, emb_labels, method='tsne')\n",
    "    \n",
    "    print(\"\\nPCA Visualization:\")\n",
    "    visualize_embeddings(track_embs, ref_embs, emb_labels, method='pca')\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Explored the footwear impression dataset\n",
    "2. Visualized augmentation techniques\n",
    "3. Analyzed model performance\n",
    "4. Examined failure cases\n",
    "5. Visualized the embedding space\n",
    "\n",
    "These insights can help us understand how the model works, what its limitations are, and how we might improve it in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
